{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15fae19-2be3-4647-82da-93a49fccaced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# thread-safe print\n",
    "from threading import Thread, Lock\n",
    "\n",
    "lock = Lock()\n",
    "def Print(*args):\n",
    "    with lock:\n",
    "        print(*args)\n",
    "        \n",
    "Print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945fc647-2397-4314-a83c-9a5123506c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic creation\n",
    "from kafka import KafkaAdminClient, KafkaProducer\n",
    "from kafka.admin import NewTopic\n",
    "broker = \"kafka:9092\"\n",
    "admin = KafkaAdminClient(bootstrap_servers=[broker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831a5265-9e19-402e-a339-1a68642cc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# admin.create_topics([NewTopic(\"animals\", 4, 1)])\n",
    "# admin.create_topics([NewTopic(\"animals-json\", 4, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9573417-1ca0-4eb5-8faa-5be10ca66ecf",
   "metadata": {},
   "source": [
    "# Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fc6f4b-4f35-49b5-b441-15bc3902ed14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGHIJ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, random, threading, json, time\n",
    "string.ascii_uppercase[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34b03a0-233f-43a7-bf1b-a666755a2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animals_pb2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99aa099f-4449-4c82-ae7e-0d222fe9c18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating animals\n"
     ]
    }
   ],
   "source": [
    "# producer\n",
    "def animal_gen():\n",
    "    Print(\"generating animals\")\n",
    "    producer = KafkaProducer(bootstrap_servers=[broker])\n",
    "    while True:\n",
    "        beach = random.choice(list(string.ascii_uppercase[:10]))\n",
    "        animal = random.choice([\"dolphin\", \"shark\", \"seagull\"])\n",
    "        key = bytes(beach, \"utf-8\")\n",
    "\n",
    "        # protobuf\n",
    "        s = Sighting(beach=beach, animal=animal)\n",
    "        value = s.SerializeToString()\n",
    "        producer.send(\"animals\", value=value, key=key)\n",
    "        \n",
    "        # JSON\n",
    "        value = {\"beach\": beach, \"animal\": animal}\n",
    "        value = bytes(json.dumps(value), \"utf-8\")\n",
    "        producer.send(\"animals-json\", value=value, key=key)\n",
    "        \n",
    "        time.sleep(1)\n",
    "threading.Thread(target=animal_gen).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8062c230-1f16-4a4d-aa9e-d97887c84362",
   "metadata": {},
   "source": [
    "# Python Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336f27b-b166-4117-a2fe-808daded48af",
   "metadata": {},
   "source": [
    "## Streaming Group By on Beach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e699bb95-e9d1-4836-88ca-a6a945c6aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "# these are TOTAL counts\n",
    "def beach_counter(partitions=[]):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", part_num) for part_num in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    \n",
    "    counts = {}\n",
    "    \n",
    "    #TODO: while True:\n",
    "    for i in range(10):\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "                if s.beach not in counts:\n",
    "                    counts[s.beach] = 0\n",
    "                counts[s.beach] += 1\n",
    "        Print(partitions, counts)\n",
    "        \n",
    "#beach_counter([0,1,2,3])\n",
    "threading.Thread(target=beach_counter, args=([0,1],)).start()\n",
    "threading.Thread(target=beach_counter, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69f8b5-c089-47b4-bd0f-b22457879f35",
   "metadata": {},
   "source": [
    "## Stream Group By on Animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3da5ee66-5e26-46ad-8ddc-81569b84d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "# these are PARTIAL counts\n",
    "def animal_counter(partitions=[]):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[broker])\n",
    "    consumer.assign([TopicPartition(\"animals\", part_num) for part_num in partitions])\n",
    "    consumer.seek_to_beginning()\n",
    "    \n",
    "    counts = {}\n",
    "    \n",
    "    #TODO: while True:\n",
    "    for i in range(10):\n",
    "        batch = consumer.poll(1000)\n",
    "        for tp, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                s = Sighting.FromString(msg.value)\n",
    "                if s.animal not in counts:\n",
    "                    counts[s.animal] = 0\n",
    "                counts[s.animal] += 1\n",
    "        Print(partitions, counts)\n",
    "        \n",
    "#beach_counter([0,1,2,3])\n",
    "threading.Thread(target=animal_counter, args=([0,1],)).start()\n",
    "threading.Thread(target=animal_counter, args=([2,3],)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56947fe-f7cf-4215-8e8f-92127891a54e",
   "metadata": {},
   "source": [
    "# Spark Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d584a9ef-fced-418a-89f7-5a52f66b4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] {'B': 168, 'I': 192, 'C': 140}\n",
      "[2, 3] {'seagull': 165, 'dolphin': 169, 'shark': 166}\n",
      "[0, 1] {'seagull': 167, 'shark': 191, 'dolphin': 142}\n",
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 299}\n",
      "[2, 3] {'G': 211, 'H': 186, 'J': 25, 'F': 38, 'A': 40}\n",
      "[2, 3] {'G': 211, 'H': 186, 'J': 188, 'F': 190, 'A': 225}\n",
      "[2, 3] {'G': 211, 'H': 186, 'J': 193, 'F': 195, 'A': 235}\n",
      "[0, 1] {'B': 190, 'I': 226, 'C': 163, 'E': 211, 'D': 199}\n",
      "[2, 3] {'seagull': 322, 'dolphin': 350, 'shark': 328}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 363, 'shark': 329}\n",
      "[2, 3] {'G': 212, 'H': 186, 'J': 193, 'F': 195, 'A': 235}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 364, 'shark': 329}\n",
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 299}\n",
      "[0, 1] {'B': 190, 'I': 226, 'C': 163, 'E': 211, 'D': 199}\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5f85f5e3-b203-41c7-8cff-f5fe90d7deca;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3] {'G': 212, 'H': 187, 'J': 193, 'F': 195, 'A': 235}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 365, 'shark': 329}\n",
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 299}\n",
      "[0, 1] {'B': 190, 'I': 226, 'C': 163, 'E': 211, 'D': 199}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1047ms :: artifacts dl 33ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5f85f5e3-b203-41c7-8cff-f5fe90d7deca\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/19ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] {'B': 190, 'I': 226, 'C': 164, 'E': 211, 'D': 199}\n",
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 300}\n",
      "[2, 3] {'G': 212, 'H': 187, 'J': 193, 'F': 195, 'A': 235}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 365, 'shark': 329}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 15:31:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 301}\n",
      "[0, 1] {'B': 190, 'I': 226, 'C': 165, 'E': 211, 'D': 199}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 365, 'shark': 329}\n",
      "[2, 3] {'G': 212, 'H': 187, 'J': 193, 'F': 195, 'A': 235}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 365, 'shark': 330}\n",
      "[0, 1] {'B': 190, 'I': 226, 'C': 165, 'E': 211, 'D': 199}\n",
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 301}\n",
      "[2, 3] {'G': 212, 'H': 187, 'J': 194, 'F': 195, 'A': 235}\n",
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 302}\n",
      "[0, 1] {'B': 191, 'I': 226, 'C': 165, 'E': 211, 'D': 199}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 365, 'shark': 330}\n",
      "[2, 3] {'G': 212, 'H': 187, 'J': 194, 'F': 195, 'A': 235}\n",
      "[0, 1] {'seagull': 324, 'shark': 366, 'dolphin': 303}\n",
      "[2, 3] {'seagull': 328, 'dolphin': 365, 'shark': 330}\n",
      "[0, 1] {'B': 192, 'I': 226, 'C': 165, 'E': 211, 'D': 199}\n",
      "[2, 3] {'G': 212, 'H': 187, 'J': 194, 'F': 195, 'A': 235}\n",
      "[0, 1] {'B': 192, 'I': 227, 'C': 165, 'E': 211, 'D': 199}\n",
      "[0, 1] {'seagull': 325, 'shark': 366, 'dolphin': 303}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .config('spark.sql.shuffle.partitions', 10)\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463169e-680f-4a8d-92b3-aa22c0aebe68",
   "metadata": {},
   "source": [
    "# regular read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea90bec2-e8c0-4380-86a9-61b110b1a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    " .option(\"subscribe\", \"animals-json\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6315d7a2-cb89-4a47-aa13-e9391561e249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e9acc3-7c6b-4f1a-90df-e868a61a42af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0e1b47e-a3bb-4f61-965a-5010fad21ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'binary'),\n",
       " ('value', 'binary'),\n",
       " ('topic', 'string'),\n",
       " ('partition', 'int'),\n",
       " ('offset', 'bigint'),\n",
       " ('timestamp', 'timestamp'),\n",
       " ('timestampType', 'int')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce668ef-6f06-4549-b4ed-eeaaef533e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "| key|               value|       topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|     0|2023-04-17 15:11:...|            0|\n",
      "|[42]|[7B 22 62 65 61 6...|animals-json|        0|     1|2023-04-17 15:11:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|     2|2023-04-17 15:11:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|     3|2023-04-17 15:11:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|     4|2023-04-17 15:12:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|     5|2023-04-17 15:12:...|            0|\n",
      "|[49]|[7B 22 62 65 61 6...|animals-json|        0|     6|2023-04-17 15:12:...|            0|\n",
      "|[49]|[7B 22 62 65 61 6...|animals-json|        0|     7|2023-04-17 15:12:...|            0|\n",
      "|[49]|[7B 22 62 65 61 6...|animals-json|        0|     8|2023-04-17 15:12:...|            0|\n",
      "|[49]|[7B 22 62 65 61 6...|animals-json|        0|     9|2023-04-17 15:12:...|            0|\n",
      "|[49]|[7B 22 62 65 61 6...|animals-json|        0|    10|2023-04-17 15:12:...|            0|\n",
      "|[49]|[7B 22 62 65 61 6...|animals-json|        0|    11|2023-04-17 15:12:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|    12|2023-04-17 15:12:...|            0|\n",
      "|[42]|[7B 22 62 65 61 6...|animals-json|        0|    13|2023-04-17 15:12:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|    14|2023-04-17 15:12:...|            0|\n",
      "|[49]|[7B 22 62 65 61 6...|animals-json|        0|    15|2023-04-17 15:12:...|            0|\n",
      "|[42]|[7B 22 62 65 61 6...|animals-json|        0|    16|2023-04-17 15:12:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|    17|2023-04-17 15:12:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|    18|2023-04-17 15:12:...|            0|\n",
      "|[43]|[7B 22 62 65 61 6...|animals-json|        0|    19|2023-04-17 15:12:...|            0|\n",
      "+----+--------------------+------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0e84df-daa5-40d1-98b5-b2d583d9e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fac11c2-d578-4171-92a7-2b82c00bb65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|key|               value|\n",
      "+---+--------------------+\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  B|{\"beach\": \"B\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  I|{\"beach\": \"I\", \"a...|\n",
      "|  I|{\"beach\": \"I\", \"a...|\n",
      "|  I|{\"beach\": \"I\", \"a...|\n",
      "|  I|{\"beach\": \"I\", \"a...|\n",
      "|  I|{\"beach\": \"I\", \"a...|\n",
      "|  I|{\"beach\": \"I\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  B|{\"beach\": \"B\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  I|{\"beach\": \"I\", \"a...|\n",
      "|  B|{\"beach\": \"B\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "|  C|{\"beach\": \"C\", \"a...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c009c8-96c6-4ae8-8e83-fe65d06595f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------+\n",
      "|key|from_json(CAST(value AS STRING))|\n",
      "+---+--------------------------------+\n",
      "|  C|                    {C, dolphin}|\n",
      "|  B|                    {B, dolphin}|\n",
      "|  C|                      {C, shark}|\n",
      "|  C|                    {C, dolphin}|\n",
      "|  C|                    {C, seagull}|\n",
      "|  C|                      {C, shark}|\n",
      "|  I|                      {I, shark}|\n",
      "|  I|                    {I, dolphin}|\n",
      "|  I|                    {I, dolphin}|\n",
      "|  I|                    {I, seagull}|\n",
      "|  I|                    {I, seagull}|\n",
      "|  I|                      {I, shark}|\n",
      "|  C|                      {C, shark}|\n",
      "|  B|                    {B, dolphin}|\n",
      "|  C|                      {C, shark}|\n",
      "|  I|                    {I, dolphin}|\n",
      "|  B|                    {B, seagull}|\n",
      "|  C|                    {C, seagull}|\n",
      "|  C|                      {C, shark}|\n",
      "|  C|                    {C, seagull}|\n",
      "+---+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ad457aa-5761-4ee2-87e6-17c6dcb47bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "|key|beach| animal|\n",
      "+---+-----+-------+\n",
      "|  C|    C|dolphin|\n",
      "|  B|    B|dolphin|\n",
      "|  C|    C|  shark|\n",
      "|  C|    C|dolphin|\n",
      "|  C|    C|seagull|\n",
      "|  C|    C|  shark|\n",
      "|  I|    I|  shark|\n",
      "|  I|    I|dolphin|\n",
      "|  I|    I|dolphin|\n",
      "|  I|    I|seagull|\n",
      "|  I|    I|seagull|\n",
      "|  I|    I|  shark|\n",
      "|  C|    C|  shark|\n",
      "|  B|    B|dolphin|\n",
      "|  C|    C|  shark|\n",
      "|  I|    I|dolphin|\n",
      "|  B|    B|seagull|\n",
      "|  C|    C|seagull|\n",
      "|  C|    C|  shark|\n",
      "|  C|    C|seagull|\n",
      "+---+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "(df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e78b9f-ad89-4d14-8887-55d78d3d5cf3",
   "metadata": {},
   "source": [
    "# readStream, writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e2dc73e-1362-4af8-8fdb-b73544e9e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read=>readStream\n",
    "df = (spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    " .option(\"subscribe\", \"animals-json\")\n",
    " .option(\"startingOffsets\", \"earliest\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fb1cbfe-60ff-498e-97ef-f1dc44aa056e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "961e8f4a-7de6-44c6-ac10-8c59b7247a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source => transformations => sink\n",
    "# query = spark.readStream(...).......writeStream(...).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1ff922c-3e8b-479b-ad87-b9fb1906d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"beach string, animal string\"\n",
    "animals = (df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "    .select(\"key\", \"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "add0f553-b452-4ea8-bb10-425622c2a0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, beach: string, animal: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8d83c32-f755-4f43-87ea-072de206eb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeaa7288-3515-4543-98bc-ca5d600b8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.streams.active[0].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41c6ee5e-5fb8-478c-b711-0fce454be87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 15:33:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4aacd6cc-5573-4d07-928e-dc641f5164d2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/17 15:33:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  H|    H| shark|\n",
      "|  G|    G| shark|\n",
      "|  G|    G| shark|\n",
      "|  G|    G| shark|\n",
      "|  H|    H| shark|\n",
      "|  H|    H| shark|\n",
      "|  H|    H| shark|\n",
      "|  G|    G| shark|\n",
      "|  G|    G| shark|\n",
      "|  H|    H| shark|\n",
      "|  G|    G| shark|\n",
      "|  G|    G| shark|\n",
      "|  G|    G| shark|\n",
      "|  H|    H| shark|\n",
      "|  G|    G| shark|\n",
      "|  H|    H| shark|\n",
      "|  H|    H| shark|\n",
      "|  G|    G| shark|\n",
      "|  H|    H| shark|\n",
      "|  H|    H| shark|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+-----+------+\n",
      "|key|beach|animal|\n",
      "+---+-----+------+\n",
      "|  D|    D| shark|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = (animals.filter(\"animal='shark'\").writeStream\n",
    " .format(\"console\").trigger(processingTime=\"5 seconds\")\n",
    " .outputMode(\"append\").start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6be922e1-2ad0-4693-96b3-df43982618f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae87b8b9-f0ce-4ffb-a663-13d06f4d1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/17 15:33:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a3e6a4c1-21d3-47b1-a19b-48b61480c0c0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/17 15:33:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  433|\n",
      "|dolphin|  424|\n",
      "|seagull|  415|\n",
      "+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  435|\n",
      "|seagull|  415|\n",
      "|dolphin|  424|\n",
      "+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  435|\n",
      "|seagull|  415|\n",
      "|dolphin|  425|\n",
      "+-------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  436|\n",
      "|seagull|  418|\n",
      "|dolphin|  426|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  438|\n",
      "|seagull|  420|\n",
      "|dolphin|  427|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  439|\n",
      "|seagull|  421|\n",
      "|dolphin|  430|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  440|\n",
      "|seagull|  424|\n",
      "|dolphin|  431|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  442|\n",
      "|seagull|  427|\n",
      "|dolphin|  431|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  444|\n",
      "|seagull|  429|\n",
      "|dolphin|  432|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  446|\n",
      "|seagull|  432|\n",
      "|dolphin|  432|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  449|\n",
      "|seagull|  434|\n",
      "|dolphin|  432|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  450|\n",
      "|seagull|  436|\n",
      "|dolphin|  434|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  452|\n",
      "|seagull|  438|\n",
      "|dolphin|  435|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  454|\n",
      "|seagull|  440|\n",
      "|dolphin|  436|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  457|\n",
      "|seagull|  441|\n",
      "|dolphin|  437|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = (animals.groupby(\"animal\").count()\n",
    " .writeStream\n",
    " .format(\"console\").trigger(processingTime=\"5 seconds\")\n",
    " .outputMode(\"complete\").start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "749e5a54-cfaa-4d85-b157-0c2a152a9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  472|\n",
      "|seagull|  448|\n",
      "|dolphin|  450|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  476|\n",
      "|seagull|  448|\n",
      "|dolphin|  451|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  478|\n",
      "|seagull|  449|\n",
      "|dolphin|  453|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+-------+-----+\n",
      "| animal|count|\n",
      "+-------+-----+\n",
      "|  shark|  481|\n",
      "|seagull|  450|\n",
      "|dolphin|  454|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q.awaitTermination(20)\n",
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88bc8e-3ea7-44d0-bb13-181370064535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
